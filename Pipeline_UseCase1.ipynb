{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, rdflib, pickle, urllib.parse, re, random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.pipeline import pipeline\n",
    "\n",
    "# Custom EHR Tools \n",
    "from EHRPipeline.entity_alignment.invokers import Invoker\n",
    "from EHRPipeline.entity_alignment.entity_alignement import CrossOntologyAligner\n",
    "from EHRPipeline.entity_alignment.embedder import SimpleDataEmbedder, ClusterGenerator\n",
    "from EHRPipeline.entity_linking.linking_validation import LinkingValidator\n",
    "from EHRPipeline.fact_validation.factValidation import Validator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General setup of environment and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "snomed_embeddings = Path(\"data/snomed_embedded.pkl\")\n",
    "clusters = Path(\"data/cluster.pkl\")\n",
    "\n",
    "if snomed_embeddings.exists():\n",
    "    with open(\"/data/snomed_embedded.pkl\", \"rb\") as file:\n",
    "        data_embedding = pickle.load(file)\n",
    "else:\n",
    "    snomed = rdflib.Graph()\n",
    "    snomed.parse(\"/data/snomed-ct-20221231-mini.ttl\", format=\"ttl\")\n",
    "    embedder = SimpleDataEmbedder(embeddingModel=tokenizer)\n",
    "    data_embedding = embedder.encode(data=snomed)\n",
    "\n",
    "if clusters.exists():\n",
    "    with open(\"data/cluster.pkl\", \"rb\") as file1:\n",
    "        segmentation = pickle.load(file1)\n",
    "else:\n",
    "    cluster = ClusterGenerator(data_embedding, n_clusters=50)\n",
    "    segmentation = cluster.generate_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENTS_CSV = \"data/mimic-iii/PATIENTS.csv\"\n",
    "DIAGNOSES_ICD_CSV = \"data/mimic-iii/DIAGNOSES_ICD.csv\"\n",
    "LABEVENTS_CSV = \"data/mimic-iii/LABEVENTS.csv\"\n",
    "DLABITEMS_CSV = \"data/mimic-iii/D_LABITEMS.csv\"\n",
    "\n",
    "OUTPUT_TTL = \"data/enhanced_sphn_triples_sample_FINAL.ttl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRACTION = 0.05  # 5%\n",
    "patients_df = pd.read_csv(PATIENTS_CSV)\n",
    "diagnoses_icd_df = pd.read_csv(DIAGNOSES_ICD_CSV)\n",
    "lab_events_df = pd.read_csv(LABEVENTS_CSV)\n",
    "lab_items_df = pd.read_csv(DLABITEMS_CSV)\n",
    "\n",
    "all_subjects = patients_df['subject_id'].unique().tolist()\n",
    "random.shuffle(all_subjects)\n",
    "\n",
    "num_5pct = int(len(all_subjects) * FRACTION)\n",
    "keep_subjects = set(all_subjects[:num_5pct])\n",
    "\n",
    "patients_df_small = patients_df[patients_df['subject_id'].isin(keep_subjects)]\n",
    "diagnoses_icd_df_small = diagnoses_icd_df[diagnoses_icd_df['subject_id'].isin(keep_subjects)]\n",
    "lab_events_df_small = lab_events_df[lab_events_df['subject_id'].isin(keep_subjects)]\n",
    "\n",
    "keep_itemids = set(lab_events_df_small['itemid'].dropna().unique())\n",
    "lab_items_df_small = lab_items_df[lab_items_df['itemid'].isin(keep_itemids)]\n",
    "\n",
    "itemid_to_loinc = {}\n",
    "for _, row in lab_items_df_small.iterrows():\n",
    "    itemid = row['itemid']\n",
    "    loinc = str(row['loinc_code']).strip()\n",
    "    if loinc == 'nan' or loinc == '':\n",
    "        loinc = None\n",
    "    itemid_to_loinc[itemid] = loinc\n",
    "\n",
    "def sanitize_value_for_iri(value):\n",
    "    \"\"\"Sanitize values for use in IRI format.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"NA\"\n",
    "    val_str = str(value)\n",
    "    return urllib.parse.quote(val_str, safe=\"-._~\")\n",
    "\n",
    "triples_ttl = []\n",
    "\n",
    "for idx, row in diagnoses_icd_df_small.iterrows():\n",
    "    row_id = row['row_id']\n",
    "    subj_id = row['subject_id']\n",
    "    icd9_code = str(row['icd9_code']).strip()\n",
    "\n",
    "    diagnosis_iri = f\"<http://example.org/Diagnosis/{subj_id}/PATIENTS/{row_id}>\"\n",
    "    subject_pseudo_iri = f\"<http://example.org/PATIENTS/{subj_id}>\"\n",
    "    icd9_iri = f\"<http://example.org/Code/icd9#{icd9_code}>\"\n",
    "\n",
    "    triples_ttl.append(f\"{diagnosis_iri} a sphn:Diagnosis .\")\n",
    "    triples_ttl.append(f\"{subject_pseudo_iri} a sphn:SubjectPseudoIdentifier .\")\n",
    "    triples_ttl.append(f\"{diagnosis_iri} sphn:hasSubjectPseudoIdentifier {subject_pseudo_iri} .\")\n",
    "    triples_ttl.append(f\"{icd9_iri} a sphn:Code .\")\n",
    "    triples_ttl.append(f\"{diagnosis_iri} sphn:hasCode {icd9_iri} .\")\n",
    "\n",
    "for idx, row in lab_events_df_small.iterrows():\n",
    "    row_id = row['row_id']\n",
    "    subj_id = row['subject_id']\n",
    "    itemid = row['itemid']\n",
    "    val = row['value']\n",
    "\n",
    "    if pd.isna(row_id) or pd.isna(subj_id) or pd.isna(itemid):\n",
    "        continue\n",
    "\n",
    "    lab_event_iri = f\"<http://example.org/LabTestEvent/{int(subj_id)}/PATIENTS/{int(row_id)}>\"\n",
    "    subject_pseudo_iri = f\"<http://example.org/PATIENTS/{int(subj_id)}>\"\n",
    "    lab_test_iri = f\"<http://example.org/LabTest/{int(subj_id)}/PATIENTS/{int(itemid)}>\"\n",
    "    \n",
    "    value_part = sanitize_value_for_iri(val)\n",
    "    lab_result_iri = f\"<http://example.org/LabResult/{int(subj_id)}/PATIENTS/{int(itemid)}/{value_part}>\"\n",
    "\n",
    "    triples_ttl.append(f\"{lab_event_iri} a sphn:LabTestEvent .\")\n",
    "    triples_ttl.append(f\"{subject_pseudo_iri} a sphn:SubjectPseudoIdentifier .\")\n",
    "    triples_ttl.append(f\"{lab_event_iri} sphn:hasSubjectPseudoIdentifier {subject_pseudo_iri} .\")\n",
    "\n",
    "    triples_ttl.append(f\"{lab_test_iri} a sphn:LabTest .\")\n",
    "    triples_ttl.append(f\"{lab_event_iri} sphn:hasLabTest {lab_test_iri} .\")\n",
    "\n",
    "    triples_ttl.append(f\"{lab_result_iri} a sphn:LabResult .\")\n",
    "    triples_ttl.append(f\"{lab_test_iri} sphn:hasResult {lab_result_iri} .\")\n",
    "\n",
    "    loinc_code = itemid_to_loinc.get(itemid, None)\n",
    "    if loinc_code is not None:\n",
    "        loinc_iri = f\"<http://example.org/Code/loinc#{loinc_code}>\"\n",
    "        triples_ttl.append(f\"{loinc_iri} a sphn:Code .\")\n",
    "        triples_ttl.append(f\"{lab_test_iri} sphn:hasCode {loinc_iri} .\")\n",
    "        triples_ttl.append(f\"{lab_result_iri} sphn:hasCode {loinc_iri} .\")\n",
    "\n",
    "with open(OUTPUT_TTL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in triples_ttl:\n",
    "        f.write(line)\n",
    "        if not line.endswith(\"\\n\"):\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "print(f\"Sample of the data has been transformed into '{OUTPUT_TTL}' with {len(triples_ttl)} RDF statements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Ontology Entity Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RDF file from schema mapping\n",
    "query = rdflib.Graph()\n",
    "query.parse(OUTPUT_TTL, format=\"ttl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontologyaligner = CrossOntologyAligner(dataGraph=data_embedding, clusters=segmentation, embeddingModel=tokenizer)\n",
    "CrossOntologyAlignedKG = ontologyaligner.merge(query=query, Invoker=\"icd9tosnomed\", Namespace=rdflib.URIRef(\"https://biomedit.ch/rdf/sphn-schema/sphn#hasCode\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Linking Validation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinkingValidator(CrossOntologyAlignedKG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransE Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_factory = TriplesFactory.from_path('data/formatted_triples_FINAL_2.txt')\n",
    "\n",
    "# Andy's code\n",
    "training, validation, testing = triples_factory.split([0.8, 0.1, 0.1])\n",
    "\n",
    "result = pipeline(\n",
    "    training=training,\n",
    "    validation=validation,\n",
    "    testing=testing,\n",
    "    model='TransE',\n",
    "    model_kwargs={\n",
    "        'embedding_dim': 20,\n",
    "    },\n",
    "    optimizer='Adam',\n",
    "    optimizer_kwargs={\n",
    "        'lr': 1e-3,\n",
    "        'weight_decay': 1e-5\n",
    "    },\n",
    "    negative_sampler='basic',\n",
    "    loss='SoftplusLoss',\n",
    "    training_loop='sLCWA',\n",
    "    training_kwargs={\n",
    "        'num_epochs': 100,\n",
    "        'batch_size': 32,\n",
    "        'label_smoothing': 0.0\n",
    "    },\n",
    "    evaluator_kwargs=  {\n",
    "        \"filtered\": True\n",
    "    },\n",
    "    filter_validation_when_testing = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen import predict  # or pykeen.models.predict, depending on version\n",
    "\n",
    "df_predictions = predict.predict_target(\n",
    "    model=result.model,\n",
    "    head=\"Diagnosis/10033/PATIENTS/112578\",\n",
    "    relation=\"hasCode\",\n",
    "    triples_factory=result.training\n",
    ").df\n",
    "\n",
    "# Inspect the top 10\n",
    "df_predictions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Fact Validation Part: Generate a txt with the previsions from the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"predictions.txt\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, row in df_predictions.head(10).iterrows():\n",
    "        predicted_code = row[\"tail_label\"] \n",
    "        subject_uri = \"<http://example.org/Diagnosis/10033/PATIENTS/112578>\"\n",
    "        predicate_uri = \"<https://biomedit.ch/rdf/sphn-schema/sphn#hasCode>\"\n",
    "        object_uri = f\"<http://example.org/Code/{predicted_code}>\"\n",
    "\n",
    "        triple_line = f\"{subject_uri}  {predicate_uri}  {object_uri}\"\n",
    "        f.write(triple_line + \"\\n\")\n",
    "\n",
    "print(f\"Wrote top-10 predictions to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sparql_endpoint = \"http://localhost:7200/repositories/integrationhealthcare\" # This is a localhost so has to be configured per machine\n",
    "    validator = Validator(sparql_endpoint)\n",
    "\n",
    "    predictions_file = \"predictions.txt\"\n",
    "    output_file = \"validated_facts.txt\"\n",
    "    \n",
    "    with open(predictions_file, \"r\", encoding=\"utf-8\") as f_in, open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for line in f_in:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Expect exactly 3 parts: subject, predicate, object\n",
    "            parts = line.split()\n",
    "            if len(parts) != 3:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "                continue\n",
    "            \n",
    "            subj = parts[0].strip()\n",
    "            pred = parts[1].strip()\n",
    "            obj  = parts[2].strip()\n",
    "\n",
    "            subj_uri = subj.strip(\"<>\")\n",
    "            pred_uri = pred.strip(\"<>\")\n",
    "            obj_uri  = obj.strip(\"<>\")\n",
    "\n",
    "            # Validate\n",
    "            score = validator.validate_fact(subj_uri, pred_uri, obj_uri, max_length=3)\n",
    "\n",
    "            print(f\"Fact: {subj} {pred} {obj} => Score: {score}\")\n",
    "\n",
    "            # threshold for writing the facts validated\n",
    "            if score >= 0.5:\n",
    "                f_out.write(f\"{subj} {pred} {obj}\\n\")\n",
    "\n",
    "    print(f\"Validation complete. Facts with score >= 0.5 are in '{output_file}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
